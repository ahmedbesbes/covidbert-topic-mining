{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook\n",
    "\n",
    "- consolidates the CORD database with external metadata from Altmetric, Scimago Journal and Cross Ref\n",
    "- generates CovidBERT embeddings from the titles and excerpts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config Completer.use_jedi=False\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(action='ignore', category=DeprecationWarning)\n",
    "\n",
    "import os\n",
    "import csv\n",
    "import glob\n",
    "import json\n",
    "import re\n",
    "import pickle\n",
    "from multiprocessing import Pool\n",
    "from IPython.display import display, Latex, HTML, FileLink\n",
    "import joblib\n",
    "import requests\n",
    "import urllib\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm_notebook\n",
    "tqdm_notebook().pandas()\n",
    "\n",
    "import semanticscholar as sch\n",
    "from langdetect import detect\n",
    "from crossref.restful import Works, Journals\n",
    "from altmetric import Altmetric\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer, models\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering, AutoModel\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, TfidfTransformer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation \n",
    "from sklearn.cluster import MiniBatchKMeans, KMeans\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score, calinski_harabasz_score\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.decomposition import PCA\n",
    "from nltk import word_tokenize          \n",
    "from nltk.stem import WordNetLemmatizer "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- put the unzipped data from kaggle inside root_path folder\n",
    "- intermediary and final results will be saved in export_path folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = \"../data/kaggle_data_v2/\"\n",
    "export_path = \"../data/exports_v3/\"\n",
    "\n",
    "if not os.path.exists(export_path):\n",
    "    os.makedirs(export_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get paths of all papers (in json format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_json = glob.glob(os.path.join(root_path, \"document_parses/**/*.json\"), recursive=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load metadata of each paper:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_df = pd.read_csv(os.path.join(root_path, 'metadata.csv'), \n",
    "                      low_memory=False, \n",
    "                      dtype={\n",
    "                            'pubmed_id': str,\n",
    "                            'Microsoft Academic Paper ID': str, \n",
    "                            'doi': str\n",
    "                        }\n",
    "                     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a FileReader class to parse each paper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FileReader:\n",
    "    def __init__(self, file_path):\n",
    "        with open(file_path) as file:\n",
    "            content = json.load(file)\n",
    "            self.paper_id = content['paper_id']\n",
    "            self.abstract = []\n",
    "            self.body_text = []\n",
    "            # Abstract\n",
    "            try:\n",
    "                for entry in content['abstract']:\n",
    "                    self.abstract.append(entry['text'])\n",
    "            except:\n",
    "                self.abstract.append(\"No abstract available\")\n",
    "            for entry in content[\"body_text\"]:\n",
    "                self.body_text.append(entry['text'])\n",
    "            self.abstract = '. '.join(self.abstract)\n",
    "            self.body_text = '. '.join(self.body_text)\n",
    "    def __repr__(self):\n",
    "        return f'{self.paper_id}: {self.abstract[:200]}... {self.body_text[:200]}...'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loop over the papers and extract information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_ = {'paper_id': [], \n",
    "         'abstract': [], \n",
    "         'body_text': [], \n",
    "         'authors': [], \n",
    "         'title': [], \n",
    "         'journal': [], \n",
    "         'publish_time': [], \n",
    "         'abstract_summary': [],\n",
    "         'doi': [],\n",
    "         'url': [],\n",
    "         'source_x': []\n",
    "        }\n",
    "\n",
    "for idx, entry in tqdm_notebook(enumerate(all_json), total=len(all_json)):\n",
    "    content = FileReader(entry)\n",
    "    \n",
    "    # get metadata information\n",
    "    meta_data = meta_df.loc[meta_df['sha'] == content.paper_id]\n",
    "    # no metadata, skip this paper\n",
    "    if len(meta_data) == 0:\n",
    "        continue\n",
    "    \n",
    "    dict_['paper_id'].append(content.paper_id)\n",
    "    dict_['abstract'].append(content.abstract)\n",
    "    dict_['body_text'].append(content.body_text)\n",
    "    \n",
    "    try:\n",
    "        authors = meta_data['authors'].values[0].split(';')\n",
    "        dict_['authors'].append(\". \".join(authors))\n",
    "    except Exception as e:\n",
    "        # if Null value\n",
    "        dict_['authors'].append(meta_data['authors'].values[0])\n",
    "    \n",
    "    # add the title information\n",
    "    dict_['title'].append(meta_data['title'].values[0])\n",
    "    \n",
    "    # add the journal information\n",
    "    dict_['journal'].append(meta_data['journal'].values[0])\n",
    "    \n",
    "    # add the publishing data\n",
    "    dict_['publish_time'].append(meta_data['publish_time'].values[0])\n",
    "    \n",
    "    # add doi \n",
    "    dict_['doi'].append(meta_data['doi'].values[0])\n",
    "    \n",
    "    # add source\n",
    "    dict_['source_x'].append(meta_data['source_x'].values[0])\n",
    "    \n",
    "    # add url\n",
    "    dict_['url'].append(meta_data['url'].values[0])    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert to dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\n",
    "    'paper_id', 'abstract', 'body_text', 'authors', 'title', \n",
    "    'journal', 'publish_time', 'doi', 'source_x', 'url'\n",
    "]\n",
    "\n",
    "df_covid = pd.DataFrame(dict_, columns=columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove duplicates in titles and null values within the body text: we keep full-text papers only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_covid.drop_duplicates(['title'], inplace=True)\n",
    "df_covid.dropna(subset=['body_text'], inplace=True)\n",
    "df_covid = df_covid[~df_covid.title.isnull()]\n",
    "df_covid.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a covid-19 for each paper based on a predefined keyword list and the publication date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covid_terms =['covid', 'coronavirus disease 19', 'sars cov 2', '2019 ncov', '2019ncov', '2019 n cov', '2019n cov',\n",
    "              'ncov 2019', 'n cov 2019', 'coronavirus 2019', 'wuhan pneumonia', 'wuhan virus', 'wuhan coronavirus',\n",
    "              'coronavirus 2', 'covid-19', 'SARS-CoV-2', '2019-nCov']\n",
    "covid_terms = [elem.lower() for elem in covid_terms]\n",
    "covid_terms = re.compile('|'.join(covid_terms))\n",
    "\n",
    "def checkYear(date):\n",
    "    return int(date[0:4])\n",
    "\n",
    "def checkCovid(row, covid_terms):\n",
    "    return bool(covid_terms.search(row['body_text'].lower())) and checkYear(row['publish_time']) > 2019\n",
    "df_covid['is_covid'] = df_covid.progress_apply(checkCovid, axis=1, covid_terms=covid_terms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Restrict to articles from 2010 +"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_covid = df_covid[df_covid.publish_time.map(lambda d: checkYear(d) >= 2010)]\n",
    "df_covid = df_covid.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean the body text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(text):\n",
    "    # remove mail\n",
    "    text = re.sub(r'[a-z0-9._%+-]+@[a-z0-9.-]+\\.[a-z]{2,}', ' ', text)\n",
    "    # remove doi\n",
    "    text = re.sub(r'https\\:\\/\\/doi\\.org[^\\s]+', 'DOI', text)\n",
    "    # remove https\n",
    "    text = re.sub(r'(\\()?\\s?http(s)?\\:\\/\\/[^\\)]+(\\))?', ' ', text)\n",
    "    # remove single characters repeated at least 3 times for spacing error (e.g. s u m m a r y)\n",
    "    text = re.sub(r'(\\w\\s+){3,}', ' ', text)\n",
    "    # replace tags (e.g. [3] [4] [5]) with whitespace\n",
    "    text = re.sub(r'(\\[\\d+\\]\\,?\\s?){3,}(\\.|\\,)?', ' ', text)\n",
    "    # replace tags (e.g. [3, 4, 5]) with whitespace\n",
    "    text = re.sub(r'\\[[\\d\\,\\s]+\\]', ' ', text)\n",
    "     # replace tags (e.g. (NUM1) repeated at least 3 times with whitespace\n",
    "    text = re.sub(r'(\\(\\d+\\)\\s){3,}', ' ', text)\n",
    "    # replace '1.3' with '1,3' (we need it for split later)\n",
    "    text = re.sub(r'(\\d+)\\.(\\d+)', ' ', text)\n",
    "    # remove all full stops as abbreviations (e.g. i.e. cit. and so on)\n",
    "    text = re.sub(r'\\.(\\s)?([^A-Z\\s])', ' \\g<1>\\g<2>', text)\n",
    "    # correctly spacing the tokens\n",
    "    text = re.sub(r' {2,}', ' ', text)\n",
    "    text = re.sub(r'\\.{2,}', '.', text)\n",
    "    # return lowercase text\n",
    "    return text.lower()\n",
    "\n",
    "df_covid['preproc_body_text'] = df_covid['body_text'].progress_apply(preprocessing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Detect the language in each paper and restrict to english papers only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_language(row):\n",
    "    try:\n",
    "        return detect(row['preproc_body_text'])\n",
    "    except:\n",
    "        try:\n",
    "            return detect(row['title'])\n",
    "        except: \n",
    "            return 'NC'\n",
    "\n",
    "df_covid['language'] = df_covid.progress_apply(detect_language, axis=1)\n",
    "df_covid = df_covid[df_covid['language'] == 'en']\n",
    "df_covid = df_covid.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_covid.to_csv(os.path.join(export_path, 'metadata.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crossref data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- crossref is an api that, given a doi, extracts additional metadata for each paper. \n",
    "- we use it to check whether the article is a preprint or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doi_list = df_covid[~df_covid['doi'].isnull()]['doi'].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scrape_crossref = True\n",
    "\n",
    "if scrape_crossref:\n",
    "    \n",
    "    works = Works(request_params={'timeout': 4})\n",
    "\n",
    "    def get_crossref_data(doi):\n",
    "        res = works.doi(doi)\n",
    "        return res\n",
    "    \n",
    "    with Pool(processes=12) as pool, tqdm_notebook(total=len(doi_list)) as pbar:\n",
    "        crossref_data = []\n",
    "        for info in pool.imap_unordered(get_crossref_data, doi_list):\n",
    "            crossref_data.append(info)\n",
    "            pbar.update()\n",
    "\n",
    "    crossref_data = [d for d in crossref_data if d is not None]\n",
    "    df_crossref = pd.DataFrame(crossref_data)\n",
    "    joblib.dump(df_crossref, os.path.join(export_path, 'crossref.joblib'))    \n",
    "    \n",
    "else:\n",
    "    df_crossref = joblib.load(os.path.join(export_path, 'crossref.joblib'))\n",
    "    \n",
    "df_covid = df_covid.merge(df_crossref[['DOI', 'subtype']], how='left', left_on='doi', right_on='DOI')\n",
    "df_covid.drop('DOI', inplace=True, axis=1)\n",
    "df_covid['preprint'] = df_covid['subtype'].map(lambda st: st == \"preprint\")\n",
    "df_covid.drop('subtype', inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_covid.to_csv(os.path.join(export_path, 'metadata.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Altmetric data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use altmetric API to fetch social metadata on each article:\n",
    "\n",
    "- readers count\n",
    "- citations in posts\n",
    "- retweets\n",
    "- citations in facebook walls\n",
    "- citations in Wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scrape_altmetrics = True\n",
    "\n",
    "if scrape_altmetrics:\n",
    "    def get_altmetric_data(doi):\n",
    "        res = a.doi(doi)\n",
    "        return res\n",
    "    \n",
    "    altmetrics_data = []\n",
    "    a = Altmetric()\n",
    "\n",
    "    for doi in tqdm_notebook(doi_list):\n",
    "        res = a.doi(doi)\n",
    "        altmetrics_data.append(res)\n",
    "\n",
    "    altmetrics_data = [d for d in altmetrics_data if d is not None]\n",
    "    \n",
    "    altmetrics_columns = [\n",
    "        'doi',\n",
    "        'score',\n",
    "        'readers_count',\n",
    "        'cited_by_posts_count',\n",
    "        'cited_by_tweeters_count',\n",
    "        'cited_by_fbwalls_count',\n",
    "        'cited_by_wikipedia_count',\n",
    "        'subjects',\n",
    "    ]\n",
    "\n",
    "    df_altmetrics = pd.DataFrame(altmetrics_data, columns=altmetrics_columns)\n",
    "    joblib.dump(df_altmetrics, os.path.join(export_path, 'altmetrics.joblib'))\n",
    "\n",
    "else:\n",
    "    df_altmetrics = joblib.load(os.path.join(export_path, 'altmetrics.joblib'))\n",
    "    \n",
    "df_covid = df_covid.merge(df_altmetrics, how='left', on='doi')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_covid.to_csv(os.path.join(export_path, 'metadata.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping H-Index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scrape the H index of each paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scrape_scimago = True\n",
    "\n",
    "if scrape_scimago:\n",
    "\n",
    "    def parse_page(url):\n",
    "        return BeautifulSoup(requests.get(url).content, 'lxml')\n",
    "    \n",
    "    journals = df_covid.journal.dropna().unique().tolist()    \n",
    "\n",
    "    def get_h_index(url):\n",
    "        soup = parse_page(url)\n",
    "        h_index_div = soup.find('div', {'class': 'hindexnumber'})\n",
    "        if h_index_div:\n",
    "            h_index = h_index_div.text\n",
    "        return h_index\n",
    "\n",
    "    def extract_info(query):\n",
    "        original_title = query\n",
    "        query = urllib.parse.quote_plus(query)\n",
    "        url = f\"https://www.scimagojr.com/journalsearch.php?q={query}\"\n",
    "        soup = parse_page(url)\n",
    "\n",
    "        search_results = soup.find('div', {'class': 'search_results'}).find_all('a')\n",
    "        if search_results == []:\n",
    "            full_title = None\n",
    "            h_index = None\n",
    "\n",
    "        else:\n",
    "            first_result = search_results[0]\n",
    "            full_title = first_result.find('span').text\n",
    "            url_journal = 'https://www.scimagojr.com/' + first_result['href']\n",
    "            h_index = get_h_index(url_journal)\n",
    "\n",
    "        info = {\n",
    "            'original_title': original_title,\n",
    "            'query': query,\n",
    "            'full_title': full_title,\n",
    "            'h_index': h_index\n",
    "        }\n",
    "        return info\n",
    "\n",
    "    with Pool(processes=12) as pool, tqdm_notebook(total=len(journals)) as pbar:\n",
    "        scimago_data = []\n",
    "        for info in pool.imap_unordered(extract_info, journals):\n",
    "            scimago_data.append(info)\n",
    "            pbar.update()\n",
    "            \n",
    "    df_scimago = pd.DataFrame(scimago_data)\n",
    "    joblib.dump(df_scimago, os.path.join(export_path, 'scimago.joblib'))\n",
    "\n",
    "else:\n",
    "    df_scimago = joblib.load(os.path.join(export_path, 'scimago.joblib'))\n",
    "    \n",
    "\n",
    "mapping_title_hindex = dict(zip(df_scimago['original_title'], df_scimago['h_index']))\n",
    "df_covid['h_index'] = df_covid.journal.map(lambda j: mapping_title_hindex[j] if j in mapping_title_hindex else j)\n",
    "df_covid['source_x'] = df_covid.source_x.map(lambda s: s.split(';')[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add a peer-reviewed tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_covid['peer_reviewed'] = df_covid['journal'].map(lambda j: False if type(j) == float else True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_covid.to_csv(os.path.join(export_path, 'metadata.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate embeddings using covid-bert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "extract an excerpt from each paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_excerpt(row):\n",
    "    if row['abstract'] == '':\n",
    "        excerpt = row['title'] + ' . ' + row['preproc_body_text'][:500]\n",
    "    else:\n",
    "        len_abstract = len(row['abstract'])\n",
    "        if len_abstract > 500:\n",
    "            excerpt = row['title'] + ' . ' + preprocessing(row['abstract'][:500])\n",
    "        else:\n",
    "            excerpt = (row['title'] + ' . ' \n",
    "                       + preprocessing(row['abstract']) + ' . ' \n",
    "                       + row['preproc_body_text'][:500 - len_abstract])\n",
    "        \n",
    "    return excerpt\n",
    "\n",
    "df_covid['excerpt'] = df_covid.progress_apply(get_excerpt, axis=1)\n",
    "df_covid.to_csv(os.path.join(export_path, 'metadata.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer(\"./src/models/covidbert/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "embed excerpts using CovidBert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "excerpt_embeddings = model.encode(df_covid.excerpt.tolist(), show_progress_bar=True, batch_size=32)\n",
    "excerpt_embeddings = np.array(excerpt_embeddings)\n",
    "np.save(os.path.join(export_path, 'embeddings_excerpts.npy'), excerpt_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "embed titles using CovidBert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles_embeddings = model.encode(df_covid.title.tolist(), show_progress_bar=True, batch_size=32)\n",
    "titles_embeddings = np.array(titles_embeddings)\n",
    "np.save(os.path.join(export_path, 'embeddings_titles.npy'), titles_embeddings)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pt1]",
   "language": "python",
   "name": "conda-env-pt1-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
